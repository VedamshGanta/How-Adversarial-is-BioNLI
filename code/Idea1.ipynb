{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "base_drive_path = '/content/drive/MyDrive/basic_benchmarking'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-BSyMTkYnoM",
        "outputId": "50aea174-33d0-40fb-cb24-fb92530800d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mofF-LcLTUEp"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import os\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import random\n",
        "import numpy as np\n",
        "import argparse\n",
        "import datetime\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "import spacy\n",
        "import re\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torch import nn\n",
        "\n",
        "# Utility function to format elapsed time\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Define utility functions\n",
        "\n",
        "def add_row(supp_set, new_line, labels_cat, original_line, data_df, index, pmid):\n",
        "    \"\"\"\n",
        "    Adds a new row to the dataframe using pd.concat instead of append.\n",
        "    \"\"\"\n",
        "    new_line = re.sub(' +', ' ', new_line)\n",
        "    row = pd.DataFrame([{\n",
        "        'supp_set': supp_set.strip(),\n",
        "        'conclusion': new_line.strip(),\n",
        "        'abstract': f\"{supp_set.strip()} <exp> {new_line.strip()}\",\n",
        "        'label_cat': labels_cat,\n",
        "        'ori_conclusion': original_line.strip(),\n",
        "        'pmid': pmid,\n",
        "        'index': index\n",
        "    }])\n",
        "    return pd.concat([data_df, row], ignore_index=True)\n",
        "\n",
        "def swap_entity_tags(sent):\n",
        "    new_sent = sent.replace('<re>', '<el1>').replace('<er>', '<le1>').replace('<el>', '<re>').replace('<le>', '<er>').replace('<el1>', '<el>').replace('<le1>', '<le>')\n",
        "    return new_sent, True\n",
        "\n",
        "def swap_entity_positions(sent):\n",
        "    try:\n",
        "        regulator = re.findall(\"<re>(.*?)<er>\", sent)[0].strip()\n",
        "        regulated = re.findall(\"<el>(.*?)<le>\", sent)[0].strip()\n",
        "        sent1 = re.sub('<re> '+re.escape(regulator)+' <er>', '<re1> '+regulated+ ' <er1>', sent)\n",
        "        sent2 = re.sub('<el> '+re.escape(regulated)+' <le>', '<re> '+regulator+ ' <er>', sent1)\n",
        "        sent3 = re.sub('<re1> '+re.escape(regulated)+ ' <er1>', '<el> '+regulated, sent2)\n",
        "    except Exception as e:\n",
        "        print('Error:', e)\n",
        "        return sent, False\n",
        "    return sent3, True\n",
        "\n",
        "def swap_entity_names(sent):\n",
        "    regulator = re.findall(\"<re>(.*?)<er>\", sent)\n",
        "    regulated = re.findall(\"<el>(.*?)<le>\", sent)\n",
        "    if not regulator or not regulated:\n",
        "        return sent, False\n",
        "    regulator = regulator[0].strip()\n",
        "    regulated = regulated[0].strip()\n",
        "    sent1 = re.sub('<re> '+re.escape(regulator)+' <er>', '<re> '+regulated+ ' <er>', sent)\n",
        "    sent2 = re.sub('<el> '+re.escape(regulated)+' <le>', '<el> '+regulator+ ' <le>', sent1)\n",
        "    return sent2, True\n",
        "\n",
        "def swapNumber(line, supp_set):\n",
        "    line_nums = re.findall(r'(?:\\d*\\.\\d+|\\d+)', line)\n",
        "    abs_nums = re.findall(r'(?:\\d*\\.\\d+|\\d+)', supp_set)\n",
        "    if not line_nums or not abs_nums:\n",
        "        return line, False\n",
        "    ln_num = random.choice(line_nums)\n",
        "    ab_num = random.choice(abs_nums)\n",
        "    new_line = line\n",
        "    if ln_num != ab_num or f'{ln_num})' in line:\n",
        "        new_line = re.sub(r'(?<!\\.)\\b{}\\b(?!\\.)'.format(re.escape(ln_num)), ab_num, line)\n",
        "    if new_line == line:\n",
        "        return new_line, False\n",
        "    return new_line, True\n",
        "\n",
        "def word_replace(sent):\n",
        "    negative = False\n",
        "    replacements = {\n",
        "        r'\\binhibits\\b': 'promotes',\n",
        "        r'\\bpromotes\\b': 'inhibits',\n",
        "        r'\\binhibition\\b': 'promotion',\n",
        "        r'\\bpromotion\\b': 'inhibition',\n",
        "        r'\\binhibitor\\b': 'promoter',\n",
        "        r'\\bpromoter\\b': 'inhibitor',\n",
        "        r'\\bincrease\\b': 'decrease',\n",
        "        r'\\bdecrease\\b': 'increase'\n",
        "    }\n",
        "    for pattern, repl in replacements.items():\n",
        "        if re.search(pattern, sent):\n",
        "            new_sent = re.sub(pattern, repl, sent, count=1)\n",
        "            return new_sent, True\n",
        "    return sent, False\n",
        "\n",
        "def posToNeg(sent):\n",
        "    negative = False\n",
        "    replacements = {\n",
        "        r'\\bwas not\\b': 'was',\n",
        "        r'\\bwere not\\b': 'were',\n",
        "        r'\\bcannot\\b': 'can',\n",
        "        r'\\bis not\\b': 'is',\n",
        "        r'\\bisn\\'t\\b': 'is',\n",
        "        r'\\bwasn\\'t\\b': 'was',\n",
        "        r'\\baren\\'t\\b': 'are',\n",
        "        r'\\bweren\\'t\\b': 'were'\n",
        "    }\n",
        "    for pattern, repl in replacements.items():\n",
        "        if re.search(pattern, sent):\n",
        "            new_sent = re.sub(pattern, repl, sent, count=1)\n",
        "            return new_sent, True\n",
        "    return sent, False\n",
        "\n",
        "def negToPos(sent):\n",
        "    negative = False\n",
        "    replacements = {\n",
        "        r'\\bwas\\b': 'was not',\n",
        "        r'\\bwere\\b': 'were not',\n",
        "        r'\\bdo\\b': 'do not',\n",
        "        r'\\bcan\\b': 'cannot',\n",
        "        r'\\bis\\b': 'is not'\n",
        "    }\n",
        "    for pattern, repl in replacements.items():\n",
        "        if re.search(pattern, sent):\n",
        "            new_sent = re.sub(pattern, repl, sent, count=1)\n",
        "            return new_sent, True\n",
        "    return sent, False\n",
        "\n",
        "def swap_random_entity_outside(sent, entity_type_map_reverse, entity_type_map):\n",
        "    regulator = re.findall(\"<re>(.*?)<er>\", sent)\n",
        "    regulated = re.findall(\"<el>(.*?)<le>\", sent)\n",
        "    if not regulator or not regulated:\n",
        "        return sent, False\n",
        "    regulator = regulator[0].strip()\n",
        "    regulated = regulated[0].strip()\n",
        "    regulator_type = entity_type_map.get(regulator.strip(), None)\n",
        "    regulated_type = entity_type_map.get(regulated.strip(), None)\n",
        "    if not regulator_type and not regulated_type:\n",
        "        return sent, False\n",
        "    if regulator_type:\n",
        "        possible_entities = entity_type_map_reverse.get(regulator_type, [])\n",
        "        new_ent = random.choice(possible_entities) if possible_entities else regulator\n",
        "        if new_ent.strip() != regulator.strip():\n",
        "            new_sent = re.sub('<re> '+re.escape(regulator)+' <er>', '<re> temppp <er>', sent)\n",
        "            new_sent = re.sub(re.escape(new_ent), regulator, new_sent)\n",
        "            new_sent = re.sub('<re> temppp <er>', f'<re> {new_ent} <er>', new_sent)\n",
        "            return new_sent, True\n",
        "    if regulated_type:\n",
        "        possible_entities = entity_type_map_reverse.get(regulated_type, [])\n",
        "        new_ent = random.choice(possible_entities) if possible_entities else regulated\n",
        "        if new_ent.strip() != regulated.strip():\n",
        "            new_sent = re.sub('<el> '+re.escape(regulated)+' <le>', '<el> tempp <le>', sent)\n",
        "            new_sent = re.sub(re.escape(new_ent), regulated, new_sent)\n",
        "            new_sent = re.sub('<el> tempp <le>', f'<el> {new_ent} <le>', new_sent)\n",
        "            return new_sent, True\n",
        "    return sent, False\n",
        "\n",
        "def generatedSamples_nd_SEN(line, gen_row):\n",
        "    try:\n",
        "        gen_exp = gen_row['generated_nd'].iloc[0][6:-16]  # Skip first <exp> token and last polarity label\n",
        "        if len(set(re.findall(\"<el>(.*?)<le>\", gen_exp))) > 1:\n",
        "            return line, False\n",
        "        if len(set(re.findall(\"<re>(.*?)<er>\", gen_exp))) > 1:\n",
        "            return line, False\n",
        "        if gen_row['sat_er_and_ed'].iloc[0] < 1:\n",
        "            return line, False\n",
        "        return gen_exp, True\n",
        "    except Exception as e:\n",
        "        print('Error occurred:', e)\n",
        "        return line, False\n",
        "\n",
        "def generatedSamples_nd_SRE(line, gen_row, model, threshold=0.94):\n",
        "    try:\n",
        "        gen_exp = gen_row['generated_nd'].iloc[0][6:-16]\n",
        "        if len(set(re.findall(\"<el>(.*?)<le>\", gen_exp))) > 1:\n",
        "            return line, False\n",
        "        if len(set(re.findall(\"<re>(.*?)<er>\", gen_exp))) > 1:\n",
        "            return line, False\n",
        "        if gen_row['sat_er_and_ed'].iloc[0] < 1:\n",
        "            return line, False\n",
        "        sentence_embeddings = model.encode([gen_exp, line])\n",
        "        if cosine_similarity([sentence_embeddings[1]], [sentence_embeddings[0]]) > 0.90:\n",
        "            return line, False\n",
        "        return gen_exp, True\n",
        "    except:\n",
        "        return line, False\n",
        "\n",
        "def generatedSamples(line, gen_row, scr_threshold=0.45, model=None):\n",
        "    gen_exp = gen_row['BM_Exp']\n",
        "    gen_lbl = gen_row['BM_lbl']\n",
        "    gen_reg = gen_row['BM_reg']\n",
        "    gen_ele = gen_row['BM_ele']\n",
        "    true_lbl = gen_row['True_lbl']\n",
        "    tru_reg = gen_row['True_reg']\n",
        "    tru_ele = gen_row['True_ele']\n",
        "    bleu_scr = gen_row['BM_scr']\n",
        "    if gen_lbl == true_lbl:\n",
        "        return line, False\n",
        "    if pd.isna(gen_reg) or pd.isna(gen_ele):\n",
        "        return line, False\n",
        "    if gen_reg != tru_reg or gen_ele != tru_ele:\n",
        "        return line, False\n",
        "    if bleu_scr > scr_threshold:\n",
        "        return line, False\n",
        "    return gen_exp, True\n"
      ],
      "metadata": {
        "id": "4pOPObrarcId"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from data_preparation.py\n",
        "\n",
        "# Define paths and parameters\n",
        "base_path = base_drive_path  # Updated to Google Drive folder\n",
        "base_save = base_drive_path   # Updated to Google Drive folder\n",
        "suffix = ''  # Modify if needed\n",
        "seq_length = 512\n",
        "tag = 'blnc'  # 'lrg' or 'blnc'; assuming 'blnc' for balanced\n",
        "\n",
        "print('Suffix is:', suffix)\n",
        "print('Tag is:', tag)\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv(os.path.join(base_path, 'train_balanced.csv'))\n",
        "dev_df = pd.read_csv(os.path.join(base_path, 'dev_balanced.csv'))\n",
        "test_df = pd.read_csv(os.path.join(base_path, 'test.csv'))\n",
        "\n",
        "print(f'Training set loaded with {len(train_df)} records.')\n",
        "print(f'Development set loaded with {len(dev_df)} records.')\n",
        "print(f'Test set loaded with {len(test_df)} records.')\n"
      ],
      "metadata": {
        "id": "Puuob51VZFMP",
        "outputId": "dfcaa8a3-123c-406b-e45a-a6e1ce471f3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suffix is: \n",
            "Tag is: blnc\n",
            "Training set loaded with 5544 records.\n",
            "Development set loaded with 12806 records.\n",
            "Test set loaded with 6308 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Load the tokenizer and add special tokens\n",
        "model_name = \"michiyasunaga/BioLinkBERT-base\"\n",
        "print('Used model is:', model_name)\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "special_tokens = ['<exp>', '<re>', '<er>', '<el>', '<le>', '<end>']\n",
        "\n",
        "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "print(f'Added {num_added_tokens} special tokens.')\n"
      ],
      "metadata": {
        "id": "EdkJaPVbTb43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23a7ee2-3973-4ce2-e0e0-b0641eff295b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used model is: michiyasunaga/BioLinkBERT-base\n",
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 6 special tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from data_preparation.py\n",
        "\n",
        "# %%\n",
        "# Define the sentence encoder\n",
        "def sentence_encoder(sentence1, sentence2, labels_map, labels=None, seq_length=128, tokenizer=tokenizer, lowercase=False):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    token_types = []\n",
        "    for i, sent1 in enumerate(sentence1):\n",
        "        sent2_text = sentence2[i] if sentence2 is not None else None\n",
        "        if lowercase:\n",
        "            sent1 = sent1.lower()\n",
        "            sent2_text = sent2_text.lower() if sent2_text else None\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            text=sent1,\n",
        "            text_pair=sent2_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=seq_length,\n",
        "            truncation=True,\n",
        "            truncation_strategy='only_first',\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "        token_types.append(encoded_dict['token_type_ids'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    token_types = torch.cat(token_types, dim=0)\n",
        "\n",
        "    try:\n",
        "        labels_out = [labels_map[c] for c in labels]\n",
        "        labels = torch.tensor(labels_out)\n",
        "    except Exception as e:\n",
        "        print(f'Error occurred during label encoding: {e}')\n",
        "        labels = None\n",
        "\n",
        "    return input_ids, attention_masks, token_types, labels\n"
      ],
      "metadata": {
        "id": "zqSxQr0tUMfp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from data_preparation.py\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n"
      ],
      "metadata": {
        "id": "1qCt0BEEUOYo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from data_preparation.py\n",
        "\n",
        "# %%\n",
        "# Define labels mapping\n",
        "labels_map = {'pos': 1}\n",
        "for item in set(train_df['label_cat'].values):\n",
        "    if item != 'pos':\n",
        "        labels_map[item] = 0\n",
        "\n",
        "print(f'Labels mapping: {labels_map}')\n"
      ],
      "metadata": {
        "id": "8Orgq6_JUQW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d0c16b-fbf4-4f6d-9679-f50ada5b3ef3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels mapping: {'pos': 1, 'posToNeg': 0, 'swap_number': 0, 'generation_nd_SRE': 0, 'generation_nd_SEN': 0, 'generation_nd': 0, 'generation': 0, 'SRE': 0, 'LPR': 0, 'SREO': 0, 'SEP': 0, 'negToPos': 0, 'SEN': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from data_preparation.py\n",
        "\n",
        "# Define dataset labeling function\n",
        "def dataset_labeling(x, p1=0.9, p2=0.7, p3=0.4, p4=0.1):\n",
        "    r = x['prob']\n",
        "    label_cat = x['label_cat']\n",
        "    if label_cat in ['generation_nd_SEN', 'generation_nd_SRE', 'generation', 'generation_nd', 'SRE']:\n",
        "        return 'train' if r < p1 else 'test'\n",
        "    elif label_cat == 'pos':\n",
        "        return 'train' if r < p2 else 'test'\n",
        "    elif label_cat in ['SEP', 'SEN', 'posToNeg', 'negToPos']:\n",
        "        return 'train' if r < p4 else 'test'\n",
        "    elif label_cat in ['LPR', 'swap_number']:\n",
        "        return 'train' if r < p3 else 'test'\n",
        "    elif label_cat == 'SREO':\n",
        "        return 'train' if r < p4 else 'test'\n",
        "    else:\n",
        "        return 'test'\n"
      ],
      "metadata": {
        "id": "YBPguuc2UR31"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Encode datasets\n",
        "train_inputs, train_masks, train_token_types, train_labels = sentence_encoder(\n",
        "    train_df['conclusion'].tolist(),\n",
        "    train_df['supp_set'].tolist(),\n",
        "    labels_map,\n",
        "    labels=train_df['label_cat'].tolist(),\n",
        "    seq_length=seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "dev_inputs, dev_masks, dev_token_types, dev_labels = sentence_encoder(\n",
        "    dev_df['conclusion'].tolist(),\n",
        "    dev_df['supp_set'].tolist(),\n",
        "    labels_map,\n",
        "    labels=dev_df['label_cat'].tolist(),\n",
        "    seq_length=seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "test_inputs, test_masks, test_token_types, test_labels = sentence_encoder(\n",
        "    test_df['conclusion'].tolist(),\n",
        "    test_df['supp_set'].tolist(),\n",
        "    labels_map,\n",
        "    labels=test_df['label_cat'].tolist(),\n",
        "    seq_length=seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(train_inputs, train_token_types, train_masks, train_labels)\n",
        "dev_dataset = TensorDataset(dev_inputs, dev_token_types, dev_masks, dev_labels)\n",
        "test_dataset = TensorDataset(test_inputs, test_token_types, test_masks, test_labels)\n",
        "\n",
        "print(f'Training dataset size: {len(train_dataset)}')\n",
        "print(f'Development dataset size: {len(dev_dataset)}')\n",
        "print(f'Test dataset size: {len(test_dataset)}')\n"
      ],
      "metadata": {
        "id": "vC8xTYaDUTgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac66492e-3c91-4ad9-f1a5-820958997dfd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 5544\n",
            "Development dataset size: 12806\n",
            "Test dataset size: 6308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "val_batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=RandomSampler(train_dataset),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    dev_dataset,\n",
        "    sampler=SequentialSampler(dev_dataset),\n",
        "    batch_size=val_batch_size\n",
        ")\n",
        "\n",
        "print(f'Training DataLoader has {len(train_dataloader)} batches.')\n",
        "print(f'Development DataLoader has {len(validation_dataloader)} batches.')\n"
      ],
      "metadata": {
        "id": "zI0CZVLdUVHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d427f4c-0d15-4198-ce7f-e77d8c84ca01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DataLoader has 347 batches.\n",
            "Development DataLoader has 801 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Install SciSpaCy and the specific NER model\n",
        "!pip install scispacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_ner_bionlp13cg_md-0.5.0.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AWXrmaoqsAD2",
        "outputId": "e997eb8f-600a-477a-dd08-c8cd3d61042d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
            "Collecting spacy<3.8.0,>=3.7.0 (from scispacy)\n",
            "  Using cached spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.13.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scispacy) (2.32.3)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (from scispacy) (6.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.26.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.5.2)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.10/dist-packages (from scispacy) (0.3.4)\n",
            "Requirement already satisfied: nmslib>=1.7.3.6 in /usr/local/lib/python3.10/dist-packages (from scispacy) (2.1.1)\n",
            "Requirement already satisfied: pybind11<2.6.2 in /usr/local/lib/python3.10/dist-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=1.7.3.6->scispacy) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.9)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8.0,>=3.7.0->scispacy)\n",
            "  Using cached thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (4.66.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (4.5.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->scispacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.2.1)\n",
            "Using cached spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Using cached thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.2.6\n",
            "    Uninstalling spacy-3.2.6:\n",
            "      Successfully uninstalled spacy-3.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-ner-bionlp13cg-md 0.5.0 requires spacy<3.3.0,>=3.2.3, but you have spacy 3.7.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.7.5 thinc-8.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy",
                  "thinc"
                ]
              },
              "id": "ab5ab9041eaa43aaa59ba53b28e1e8f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_ner_bionlp13cg_md-0.5.0.tar.gz\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_ner_bionlp13cg_md-0.5.0.tar.gz (120.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.3.0,>=3.2.3 (from en_ner_bionlp13cg_md==0.5.0)\n",
            "  Using cached spacy-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (3.0.9)\n",
            "Collecting thinc<8.1.0,>=8.0.12 (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0)\n",
            "  Using cached thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (0.7.11)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (4.5.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (0.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (2024.8.30)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.3->en_ner_bionlp13cg_md==0.5.0) (1.2.1)\n",
            "Using cached spacy-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
            "Using cached thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.2.6 which is incompatible.\n",
            "scispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.2.6 thinc-8.0.17\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy",
                  "thinc"
                ]
              },
              "id": "1a0e4fabeccd4e68bd2e4274e7018b0d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Import SpaCy and initialize the NER model\n",
        "import spacy\n",
        "\n",
        "def extract_and_save_entities(sentences, save_path):\n",
        "    entity_types = []\n",
        "    for idx, sent in enumerate(sentences):\n",
        "        if idx % 1000 == 0:\n",
        "            print(f'Processing sentence {idx}')\n",
        "        doc = nlp(sent)\n",
        "        entities = {ent.text: ent.label_ for ent in doc.ents}\n",
        "        entity_types.append(entities)\n",
        "\n",
        "    # Save to JSON and CSV\n",
        "    with open(save_path + '.json', 'w') as f_json, open(save_path + '.csv', 'w') as f_csv:\n",
        "        for entities in entity_types:\n",
        "            json.dump(entities, f_json)\n",
        "            f_json.write('\\n')\n",
        "        pd.DataFrame(entity_types).to_csv(f_csv, index=False)\n",
        "\n",
        "    print(f'Entities saved to {save_path}.json and {save_path}.csv')\n",
        "\n",
        "# Initialize SpaCy NER model\n",
        "nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
        "\n",
        "# Extract entities from training and development sets\n",
        "extract_and_save_entities(train_df['conclusion'].tolist(), os.path.join(base_save, 'conclusion_entity_types'))\n",
        "extract_and_save_entities(train_df['supp_set'].tolist(), os.path.join(base_save, 'supp_set_entity_types'))\n"
      ],
      "metadata": {
        "id": "bx9CYYUUUX87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2e291e-0f9c-417f-be7f-47052b0ac9d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing sentence 0\n",
            "Processing sentence 1000\n",
            "Processing sentence 2000\n",
            "Processing sentence 3000\n",
            "Processing sentence 4000\n",
            "Processing sentence 5000\n",
            "Entities saved to /content/drive/MyDrive/basic_benchmarking/conclusion_entity_types.json and /content/drive/MyDrive/basic_benchmarking/conclusion_entity_types.csv\n",
            "Processing sentence 0\n",
            "Processing sentence 1000\n",
            "Processing sentence 2000\n",
            "Processing sentence 3000\n",
            "Processing sentence 4000\n",
            "Processing sentence 5000\n",
            "Entities saved to /content/drive/MyDrive/basic_benchmarking/supp_set_entity_types.json and /content/drive/MyDrive/basic_benchmarking/supp_set_entity_types.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Combine train and dev datasets for further processing\n",
        "combined_input = pd.concat([train_df, dev_df], ignore_index=True)\n",
        "\n",
        "print(f'Combined input dataset size: {len(combined_input)}')\n"
      ],
      "metadata": {
        "id": "tKacpqpEUZv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24de3d7b-b41d-471d-d97d-2c12628e2058"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined input dataset size: 18350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Initialize counts and storage variables\n",
        "LPR_count = posToNeg_count = negToPos_count = swap_number_count = generation_count = 0\n",
        "generation_nd_count = generation_nd_SRE_count = generation_nd_SEN_count = 0\n",
        "SRE_count = SEP_count = SET_count = SEN_count = SREO_count = 0\n",
        "negative_sents = []\n",
        "positive_sents = []\n",
        "negative_ids = []\n",
        "positive_ids = []\n",
        "negative_supp_set = []\n",
        "positive_supp_set = []\n",
        "labels = []\n",
        "data_df = pd.DataFrame(columns=['supp_set', 'conclusion', 'abstract', 'label_cat', 'ori_conclusion', 'pmid', 'index'])\n"
      ],
      "metadata": {
        "id": "iNY1I2FTolR_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Load entity type mappings\n",
        "entity_type_map_reverse = {}\n",
        "entity_type_map = {}\n",
        "with open(os.path.join(base_save, 'supp_set_entity_types.json')) as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        supp_set_entity_type = json.loads(line)\n",
        "        for ent, label in supp_set_entity_type.items():\n",
        "            entity_type_map[ent] = label\n",
        "            entity_type_map_reverse.setdefault(label, []).append(ent)\n",
        "\n",
        "print(f'Loaded {len(entity_type_map)} entity types.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyxmVclgqQTw",
        "outputId": "40f8d59b-d69d-4452-a290-6f44bed69209"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 38758 entity types.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from entailment_preparation.ipynb\n",
        "\n",
        "# Initialize a list to collect all rows\n",
        "rows = []\n",
        "\n",
        "# Iterate over each example to create positive and negative samples\n",
        "for index, (line, supp_set, pmid) in enumerate(zip(combined_input['conclusion'], combined_input['supp_set'], combined_input['pmid'])):\n",
        "    if index % 1000 == 0:\n",
        "        print(f'Processed {index} items')\n",
        "\n",
        "    if pd.isna(line) or pd.isna(supp_set) or not re.search(\"<re>(.*?)<er>\", line) or not re.search(\"<el>(.*?)<le>\", line):\n",
        "        continue\n",
        "\n",
        "    # Add positive example\n",
        "    positive_sents.append(line)\n",
        "    positive_supp_set.append(supp_set)\n",
        "    positive_ids.append(pmid)\n",
        "    labels_cat = 'pos'\n",
        "    labels.append(labels_cat)  # Append label\n",
        "    row = {\n",
        "        'supp_set': supp_set.strip(),\n",
        "        'conclusion': re.sub(' +', ' ', line).strip(),\n",
        "        'abstract': f\"{supp_set.strip()} <exp> {re.sub(' +', ' ', line).strip()}\",\n",
        "        'label_cat': labels_cat,\n",
        "        'ori_conclusion': line.strip(),\n",
        "        'pmid': pmid,\n",
        "        'index': index\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "    # Generate and add negative examples\n",
        "    negative_supp_set.append(supp_set)\n",
        "    negative_ids.append(pmid)\n",
        "\n",
        "    # 1. Swap Random Entity Outside\n",
        "    new_line, negative_c = swap_random_entity_outside(line, entity_type_map_reverse, entity_type_map)\n",
        "    if negative_c:\n",
        "        labels_cat = 'SREO'\n",
        "        negative_sents.append(new_line)\n",
        "        labels.append(labels_cat)\n",
        "        SREO_count +=1\n",
        "        row = {\n",
        "            'supp_set': supp_set.strip(),\n",
        "            'conclusion': re.sub(' +', ' ', new_line).strip(),\n",
        "            'abstract': f\"{supp_set.strip()} <exp> {re.sub(' +', ' ', new_line).strip()}\",\n",
        "            'label_cat': labels_cat,\n",
        "            'ori_conclusion': line.strip(),\n",
        "            'pmid': pmid,\n",
        "            'index': index\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # 2. Swap Entity Positions\n",
        "    new_line, negative_c = swap_entity_positions(line)\n",
        "    if negative_c:\n",
        "        labels_cat = 'SEP'\n",
        "        negative_sents.append(new_line)\n",
        "        labels.append(labels_cat)\n",
        "        SEP_count +=1\n",
        "        row = {\n",
        "            'supp_set': supp_set.strip(),\n",
        "            'conclusion': re.sub(' +', ' ', new_line).strip(),\n",
        "            'abstract': f\"{supp_set.strip()} <exp> {re.sub(' +', ' ', new_line).strip()}\",\n",
        "            'label_cat': labels_cat,\n",
        "            'ori_conclusion': line.strip(),\n",
        "            'pmid': pmid,\n",
        "            'index': index\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # 3. Swap Entity Tags\n",
        "    new_line, negative_c = swap_entity_tags(line)\n",
        "    if negative_c:\n",
        "        labels_cat = 'SET'\n",
        "        negative_sents.append(new_line)\n",
        "        labels.append(labels_cat)\n",
        "        SET_count +=1\n",
        "        row = {\n",
        "            'supp_set': supp_set.strip(),\n",
        "            'conclusion': re.sub(' +', ' ', new_line).strip(),\n",
        "            'abstract': f\"{supp_set.strip()} <exp> {re.sub(' +', ' ', new_line).strip()}\",\n",
        "            'label_cat': labels_cat,\n",
        "            'ori_conclusion': line.strip(),\n",
        "            'pmid': pmid,\n",
        "            'index': index\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # 4. Swap Entity Names\n",
        "    new_line, negative_c = swap_entity_names(line)\n",
        "    if negative_c:\n",
        "        labels_cat = 'SEN'\n",
        "        negative_sents.append(new_line)\n",
        "        labels.append(labels_cat)\n",
        "        SEN_count +=1\n",
        "        row = {\n",
        "            'supp_set': supp_set.strip(),\n",
        "            'conclusion': re.sub(' +', ' ', new_line).strip(),\n",
        "            'abstract': f\"{supp_set.strip()} <exp> {re.sub(' +', ' ', new_line).strip()}\",\n",
        "            'label_cat': labels_cat,\n",
        "            'ori_conclusion': line.strip(),\n",
        "            'pmid': pmid,\n",
        "            'index': index\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # 5. Swap Number\n",
        "    new_line, negative_c = swapNumber(line, supp_set)\n",
        "    if negative_c:\n",
        "        labels_cat = 'swap_number'\n",
        "        negative_sents.append(new_line)\n",
        "        labels.append(labels_cat)\n",
        "        swap_number_count +=1\n",
        "        row = {\n",
        "            'supp_set': supp_set.strip(),\n",
        "            'conclusion': re.sub(' +', ' ', new_line).strip(),\n",
        "            'abstract': f\"{supp_set.strip()} <exp> {re.sub(' +', ' ', new_line).strip()}\",\n",
        "            'label_cat': labels_cat,\n",
        "            'ori_conclusion': line.strip(),\n",
        "            'pmid': pmid,\n",
        "            'index': index\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # 6. Word Replace\n",
        "    new_line, negative_c = word_replace(line)\n",
        "    if negative_c:\n",
        "        labels_cat = 'LPR'\n",
        "        negative_sents.append(new_line)\n",
        "        labels.append(labels_cat)\n",
        "        LPR_count +=1\n",
        "        row = {\n",
        "            'supp_set': supp_set.strip(),\n",
        "            'conclusion': re.sub(' +', ' ', new_line).strip(),\n",
        "            'abstract': f\"{supp_set.strip()} <exp> {re.sub(' +', ' ', new_line).strip()}\",\n",
        "            'label_cat': labels_cat,\n",
        "            'ori_conclusion': line.strip(),\n",
        "            'pmid': pmid,\n",
        "            'index': index\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # 7. posToNeg\n",
        "    new_line, negative_c = posToNeg(line)\n",
        "    if negative_c:\n",
        "        labels_cat = 'posToNeg'\n",
        "        negative_sents.append(new_line)\n",
        "        labels.append(labels_cat)\n",
        "        posToNeg_count +=1\n",
        "        row = {\n",
        "            'supp_set': supp_set.strip(),\n",
        "            'conclusion': re.sub(' +', ' ', new_line).strip(),\n",
        "            'abstract': f\"{supp_set.strip()} <exp> {re.sub(' +', ' ', new_line).strip()}\",\n",
        "            'label_cat': labels_cat,\n",
        "            'ori_conclusion': line.strip(),\n",
        "            'pmid': pmid,\n",
        "            'index': index\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # 8. negToPos\n",
        "    new_line, negative_c = negToPos(line)\n",
        "    if negative_c:\n",
        "        labels_cat = 'negToPos'\n",
        "        negative_sents.append(new_line)\n",
        "        labels.append(labels_cat)\n",
        "        negToPos_count +=1\n",
        "        row = {\n",
        "            'supp_set': supp_set.strip(),\n",
        "            'conclusion': re.sub(' +', ' ', new_line).strip(),\n",
        "            'abstract': f\"{supp_set.strip()} <exp> {re.sub(' +', ' ', new_line).strip()}\",\n",
        "            'label_cat': labels_cat,\n",
        "            'ori_conclusion': line.strip(),\n",
        "            'pmid': pmid,\n",
        "            'index': index\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # Optionally, save intermediate results every 1000 items\n",
        "    if index % 1000 == 0 and index != 0:\n",
        "        temp_df = pd.DataFrame(rows)\n",
        "        temp_df.to_csv(os.path.join(base_path, 'entailment_data.csv'), mode='a', index=False, header=not os.path.exists(os.path.join(base_path, 'entailment_data.csv')))\n",
        "\n",
        "        with open(os.path.join(base_path, 'positive_conclusion.txt'), 'a') as f:\n",
        "            for item in positive_sents:\n",
        "                f.write(f\"{item}\\n\")\n",
        "        with open(os.path.join(base_path, 'negative_conclusion.txt'), 'a') as f:\n",
        "            for item in negative_sents:\n",
        "                f.write(f\"{item}\\n\")\n",
        "        with open(os.path.join(base_path, 'positive_suppset.txt'), 'a') as f:\n",
        "            for item in positive_supp_set:\n",
        "                f.write(f\"{item}\\n\")\n",
        "        with open(os.path.join(base_path, 'negative_suppset.txt'), 'a') as f:\n",
        "            for item in negative_supp_set:\n",
        "                f.write(f\"{item}\\n\")\n",
        "        with open(os.path.join(base_save, 'negative_ids.pk'), 'wb') as f:\n",
        "            pickle.dump(negative_ids, f)\n",
        "        with open(os.path.join(base_save, 'positive_ids.pk'), 'wb') as f:\n",
        "            pickle.dump(positive_ids, f)\n",
        "        with open(os.path.join(base_save, 'labels_cat.pk'), 'wb') as f:\n",
        "            pickle.dump(labels, f)\n",
        "\n",
        "        # Clear the rows list after saving to avoid duplication\n",
        "        rows = []\n",
        "\n",
        "# After the loop, save any remaining rows\n",
        "if rows:\n",
        "    final_df = pd.DataFrame(rows)\n",
        "    final_df.to_csv(os.path.join(base_path, 'entailment_data.csv'), mode='a', index=False, header=not os.path.exists(os.path.join(base_path, 'entailment_data.csv')))\n",
        "\n",
        "# Similarly, save the remaining lists if needed\n",
        "with open(os.path.join(base_path, 'positive_conclusion.txt'), 'a') as f:\n",
        "    for item in positive_sents:\n",
        "        f.write(f\"{item}\\n\")\n",
        "with open(os.path.join(base_path, 'negative_conclusion.txt'), 'a') as f:\n",
        "    for item in negative_sents:\n",
        "        f.write(f\"{item}\\n\")\n",
        "with open(os.path.join(base_path, 'positive_suppset.txt'), 'a') as f:\n",
        "    for item in positive_supp_set:\n",
        "        f.write(f\"{item}\\n\")\n",
        "with open(os.path.join(base_path, 'negative_suppset.txt'), 'a') as f:\n",
        "    for item in negative_supp_set:\n",
        "        f.write(f\"{item}\\n\")\n",
        "with open(os.path.join(base_save, 'negative_ids.pk'), 'wb') as f:\n",
        "    pickle.dump(negative_ids, f)\n",
        "with open(os.path.join(base_save, 'positive_ids.pk'), 'wb') as f:\n",
        "    pickle.dump(positive_ids, f)\n",
        "with open(os.path.join(base_save, 'labels_cat.pk'), 'wb') as f:\n",
        "    pickle.dump(labels, f)\n"
      ],
      "metadata": {
        "id": "4PpL5t_dUc9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77153501-30e3-4279-a100-3d0a8a68e7ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 items\n",
            "Processed 1000 items\n",
            "Processed 2000 items\n",
            "Processed 3000 items\n",
            "Processed 4000 items\n",
            "Processed 5000 items\n",
            "Processed 6000 items\n",
            "Processed 7000 items\n",
            "Processed 8000 items\n",
            "Processed 9000 items\n",
            "Processed 10000 items\n",
            "Processed 11000 items\n",
            "Processed 12000 items\n",
            "Processed 13000 items\n",
            "Processed 14000 items\n",
            "Processed 15000 items\n",
            "Processed 16000 items\n",
            "Processed 17000 items\n",
            "Processed 18000 items\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from classifier.py\n",
        "\n",
        "# Initialize the model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")\n",
        "\n",
        "# Resize token embeddings to accommodate new special tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Move the model to the specified device\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "oKBKjGugUvdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f6442d-536f-4ab7-8a94-038274565b99"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28901, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the optimizer and scheduler\n",
        "epochs = 3\n",
        "learning_rate = 2e-5\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "metadata": {
        "id": "mkXQ4VxwUxI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff226a4-ab2c-465a-baa4-c3bdeea4cc75"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed value for reproducibility\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n"
      ],
      "metadata": {
        "id": "c9_Onp79yC_B"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from classifier.py\n",
        "\n",
        "# Freeze certain layers for efficiency\n",
        "for name, param in model.named_parameters():\n",
        "    if 'classifier' not in name and 'cls' not in name and \\\n",
        "       'layer.11' not in name and 'layer.10' not in name and 'layer.9' not in name:\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "uoFW0uLLU0eu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from classifier.py\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(dataloader, model, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    for batch in dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_token_types = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                token_type_ids=b_token_types,\n",
        "                attention_mask=b_input_mask\n",
        "            )\n",
        "        logits = outputs.logits\n",
        "        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        true_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "    report = classification_report(true_labels, preds, labels=[0,1])\n",
        "    conf_matrix = confusion_matrix(true_labels, preds, labels=[0,1])\n",
        "    acc = accuracy_score(true_labels, preds)\n",
        "    macro_f1 = f1_score(true_labels, preds, labels=[0,1], average='macro')\n",
        "\n",
        "    print(report)\n",
        "    print(conf_matrix)\n",
        "    print(f'Macro F1-score: {macro_f1:.2f}')\n",
        "\n",
        "    return acc, macro_f1\n"
      ],
      "metadata": {
        "id": "oDC05Oo0U27a"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Code from classifier.py\n",
        "\n",
        "# Training loop\n",
        "training_stats = []\n",
        "best_macro_f1 = 0\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n======== Epoch {epoch + 1} / {epochs} ========\")\n",
        "    print(\"Training...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 100 == 0 and step != 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print(f'  Batch {step} of {len(train_dataloader)}. Elapsed: {elapsed}.')\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_token_types = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            b_input_ids,\n",
        "            token_type_ids=b_token_types,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(f\"\\n  Average training loss: {avg_train_loss:.2f}\")\n",
        "    print(f\"  Training epoch took: {training_time}\")\n",
        "\n",
        "    # Validation\n",
        "    print(\"\\nRunning Validation...\")\n",
        "    acc, macro_f1 = evaluate(validation_dataloader, model, device)\n",
        "\n",
        "    # Save the model if it has the best macro F1\n",
        "    if macro_f1 > best_macro_f1:\n",
        "        best_macro_f1 = macro_f1\n",
        "        save_path = os.path.join(base_save, f'{model_name[:5]}_2sent_class_layer_{tag}{suffix}_top3')\n",
        "        model.save_pretrained(save_path)\n",
        "        tokenizer.save_pretrained(save_path)\n",
        "        print(f'New best model saved with Macro F1: {best_macro_f1:.3f}')\n",
        "\n",
        "    # Record statistics\n",
        "    training_stats.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'avg_train_loss': avg_train_loss,\n",
        "        'training_time': training_time,\n",
        "        'validation_accuracy': acc,\n",
        "        'validation_macro_f1': macro_f1\n",
        "    })\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "total_training_time = format_time(time.time() - total_t0)\n",
        "print(f\"Total training time: {total_training_time}\")\n",
        "print(f\"Best Macro F1 achieved: {best_macro_f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "helcrmdfU4mi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b09c35-4203-4918-c448-4c46c78dc000"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch 100 of 347. Elapsed: 0:01:14.\n",
            "  Batch 200 of 347. Elapsed: 0:02:27.\n",
            "  Batch 300 of 347. Elapsed: 0:03:40.\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epoch took: 0:04:14\n",
            "\n",
            "Running Validation...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.58      0.73      9986\n",
            "           1       0.38      0.91      0.54      2820\n",
            "\n",
            "    accuracy                           0.66     12806\n",
            "   macro avg       0.67      0.75      0.63     12806\n",
            "weighted avg       0.83      0.66      0.68     12806\n",
            "\n",
            "[[5829 4157]\n",
            " [ 251 2569]]\n",
            "Macro F1-score: 0.63\n",
            "New best model saved with Macro F1: 0.632\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch 100 of 347. Elapsed: 0:01:13.\n",
            "  Batch 200 of 347. Elapsed: 0:02:25.\n",
            "  Batch 300 of 347. Elapsed: 0:03:38.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0:04:12\n",
            "\n",
            "Running Validation...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.69      0.81      9986\n",
            "           1       0.47      0.96      0.63      2820\n",
            "\n",
            "    accuracy                           0.75     12806\n",
            "   macro avg       0.73      0.83      0.72     12806\n",
            "weighted avg       0.87      0.75      0.77     12806\n",
            "\n",
            "[[6892 3094]\n",
            " [ 110 2710]]\n",
            "Macro F1-score: 0.72\n",
            "New best model saved with Macro F1: 0.720\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch 100 of 347. Elapsed: 0:01:13.\n",
            "  Batch 200 of 347. Elapsed: 0:02:25.\n",
            "  Batch 300 of 347. Elapsed: 0:03:38.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0:04:12\n",
            "\n",
            "Running Validation...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.82      0.88      9986\n",
            "           1       0.58      0.88      0.70      2820\n",
            "\n",
            "    accuracy                           0.83     12806\n",
            "   macro avg       0.77      0.85      0.79     12806\n",
            "weighted avg       0.88      0.83      0.84     12806\n",
            "\n",
            "[[8183 1803]\n",
            " [ 333 2487]]\n",
            "Macro F1-score: 0.79\n",
            "New best model saved with Macro F1: 0.792\n",
            "\n",
            "Training complete!\n",
            "Total training time: 0:31:56\n",
            "Best Macro F1 achieved: 0.792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Create model_predictions.csv\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path where the best model is saved\n",
        "best_model_path = os.path.join(base_save, f'{model_name[:5]}_2sent_class_layer_{tag}{suffix}_top3')\n",
        "\n",
        "# Check if the best model exists\n",
        "if not os.path.exists(best_model_path):\n",
        "    raise FileNotFoundError(f\"Best model not found at {best_model_path}. Please ensure the model was saved correctly.\")\n",
        "\n",
        "# Load the best model\n",
        "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
        "best_model.to(device)\n",
        "best_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize lists to store predictions and true labels\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "# Collect premises and hypotheses from dev_df\n",
        "premises = dev_df['supp_set'].values\n",
        "hypotheses = dev_df['conclusion'].values\n",
        "\n",
        "# Iterate over the validation DataLoader and collect predictions\n",
        "for batch in validation_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_token_types = batch[1].to(device)\n",
        "    b_input_mask = batch[2].to(device)\n",
        "    b_labels_batch = batch[3].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = best_model(\n",
        "            b_input_ids,\n",
        "            token_type_ids=b_token_types,\n",
        "            attention_mask=b_input_mask\n",
        "        )\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    predicted_labels.extend(predictions)\n",
        "    true_labels.extend(b_labels_batch.cpu().numpy())\n",
        "\n",
        "# Define inverse label mapping for readability\n",
        "# Assuming labels_map = {'pos': 1, 'neg_type1': 0, 'neg_type2': 0, ...}\n",
        "label_map_inv = {1: 'pos', 0: 'neg'}\n",
        "\n",
        "# Map predicted labels to string labels\n",
        "predicted_labels_mapped = [label_map_inv.get(label, 'unknown') for label in predicted_labels]\n",
        "\n",
        "# Map true labels to string labels\n",
        "# Since true labels in dev_df['label_cat'] are strings, we keep them as is\n",
        "true_labels_mapped = dev_df['label_cat'].values\n",
        "\n",
        "# Create the DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'premise': premises,\n",
        "    'hypothesis': hypotheses,\n",
        "    'predicted_label': predicted_labels_mapped,\n",
        "    'true_label': true_labels_mapped\n",
        "})\n",
        "\n",
        "# Optional: Display first few rows to verify\n",
        "print(results_df.head())\n",
        "\n",
        "# Define the path to save the CSV\n",
        "results_csv_path = os.path.join(base_save, 'model_predictions.csv')\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "results_df.to_csv(results_csv_path, index=False)\n",
        "print(f\"Model predictions saved to {results_csv_path}\")\n"
      ],
      "metadata": {
        "id": "C7pF25lzVD8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea52240-b8c8-42a9-97ee-ea036ce8ba8b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             premise  \\\n",
            "0  CaCl2 suppresses the plasma renin activity (PR...   \n",
            "1  CaCl2 suppresses the plasma renin activity (PR...   \n",
            "2  CaCl2 suppresses the plasma renin activity (PR...   \n",
            "3  CaCl2 suppresses the plasma renin activity (PR...   \n",
            "4  CaCl2 suppresses the plasma renin activity (PR...   \n",
            "\n",
            "                                          hypothesis predicted_label  \\\n",
            "0  In conclusion, promotion of <re> PRA <er> by <...             neg   \n",
            "1  In conclusion, inhibition of <el> CaCl2 <le> b...             neg   \n",
            "2  In conclusion, inhibition of <re> PRA <er> by ...             neg   \n",
            "3  In conclusion, inhibition of <re> PRA <er> by ...             neg   \n",
            "4  In conclusion, inhibition of <re> CaCl2 <er> b...             neg   \n",
            "\n",
            "  true_label  \n",
            "0        LPR  \n",
            "1        SEP  \n",
            "2        pos  \n",
            "3   posToNeg  \n",
            "4        SEN  \n",
            "Model predictions saved to /content/drive/MyDrive/basic_benchmarking/model_predictions.csv\n"
          ]
        }
      ]
    }
  ]
}